{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJQaBVcPt185oturPXeuaD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arthi26A/NLP/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm5Ont83YfMa",
        "outputId": "c619fb94-eb4c-4029-92d7-51941e10451a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams:\n",
            "Counter({('is',): 4, ('of',): 3, ('the',): 3, (',',): 3, ('natural',): 2, ('language',): 2, ('(',): 2, ('nlp',): 2, (')',): 2, ('a',): 2, ('that',): 2, ('and',): 2, ('.',): 2, ('processing',): 1, ('sub-field',): 1, ('artificial',): 1, ('intelligence',): 1, ('ai',): 1, ('focused',): 1, ('on',): 1, ('interaction',): 1, ('between',): 1, ('computers',): 1, ('humans',): 1, ('through',): 1, ('ultimate',): 1, ('objective',): 1, ('to',): 1, ('read',): 1, ('decipher',): 1, ('understand',): 1, ('make',): 1, ('sense',): 1, ('human',): 1, ('languages',): 1, ('in',): 1, ('manner',): 1, ('valuable',): 1})\n",
            "\n",
            "Bigrams:\n",
            "Counter({('natural', 'language'): 2, ('that', 'is'): 2, ('language', 'processing'): 1, ('processing', '('): 1, ('(', 'nlp'): 1, ('nlp', ')'): 1, (')', 'is'): 1, ('is', 'a'): 1, ('a', 'sub-field'): 1, ('sub-field', 'of'): 1, ('of', 'artificial'): 1, ('artificial', 'intelligence'): 1, ('intelligence', '('): 1, ('(', 'ai'): 1, ('ai', ')'): 1, (')', 'that'): 1, ('is', 'focused'): 1, ('focused', 'on'): 1, ('on', 'the'): 1, ('the', 'interaction'): 1, ('interaction', 'between'): 1, ('between', 'computers'): 1, ('computers', 'and'): 1, ('and', 'humans'): 1, ('humans', 'through'): 1, ('through', 'natural'): 1, ('language', '.'): 1, ('.', 'the'): 1, ('the', 'ultimate'): 1, ('ultimate', 'objective'): 1, ('objective', 'of'): 1, ('of', 'nlp'): 1, ('nlp', 'is'): 1, ('is', 'to'): 1, ('to', 'read'): 1, ('read', ','): 1, (',', 'decipher'): 1, ('decipher', ','): 1, (',', 'understand'): 1, ('understand', ','): 1, (',', 'and'): 1, ('and', 'make'): 1, ('make', 'sense'): 1, ('sense', 'of'): 1, ('of', 'the'): 1, ('the', 'human'): 1, ('human', 'languages'): 1, ('languages', 'in'): 1, ('in', 'a'): 1, ('a', 'manner'): 1, ('manner', 'that'): 1, ('is', 'valuable'): 1, ('valuable', '.'): 1})\n",
            "\n",
            "Trigrams:\n",
            "Counter({('natural', 'language', 'processing'): 1, ('language', 'processing', '('): 1, ('processing', '(', 'nlp'): 1, ('(', 'nlp', ')'): 1, ('nlp', ')', 'is'): 1, (')', 'is', 'a'): 1, ('is', 'a', 'sub-field'): 1, ('a', 'sub-field', 'of'): 1, ('sub-field', 'of', 'artificial'): 1, ('of', 'artificial', 'intelligence'): 1, ('artificial', 'intelligence', '('): 1, ('intelligence', '(', 'ai'): 1, ('(', 'ai', ')'): 1, ('ai', ')', 'that'): 1, (')', 'that', 'is'): 1, ('that', 'is', 'focused'): 1, ('is', 'focused', 'on'): 1, ('focused', 'on', 'the'): 1, ('on', 'the', 'interaction'): 1, ('the', 'interaction', 'between'): 1, ('interaction', 'between', 'computers'): 1, ('between', 'computers', 'and'): 1, ('computers', 'and', 'humans'): 1, ('and', 'humans', 'through'): 1, ('humans', 'through', 'natural'): 1, ('through', 'natural', 'language'): 1, ('natural', 'language', '.'): 1, ('language', '.', 'the'): 1, ('.', 'the', 'ultimate'): 1, ('the', 'ultimate', 'objective'): 1, ('ultimate', 'objective', 'of'): 1, ('objective', 'of', 'nlp'): 1, ('of', 'nlp', 'is'): 1, ('nlp', 'is', 'to'): 1, ('is', 'to', 'read'): 1, ('to', 'read', ','): 1, ('read', ',', 'decipher'): 1, (',', 'decipher', ','): 1, ('decipher', ',', 'understand'): 1, (',', 'understand', ','): 1, ('understand', ',', 'and'): 1, (',', 'and', 'make'): 1, ('and', 'make', 'sense'): 1, ('make', 'sense', 'of'): 1, ('sense', 'of', 'the'): 1, ('of', 'the', 'human'): 1, ('the', 'human', 'languages'): 1, ('human', 'languages', 'in'): 1, ('languages', 'in', 'a'): 1, ('in', 'a', 'manner'): 1, ('a', 'manner', 'that'): 1, ('manner', 'that', 'is'): 1, ('that', 'is', 'valuable'): 1, ('is', 'valuable', '.'): 1})\n",
            "\n",
            "Bigram Probabilities:\n",
            "P(language|natural) = 1.0000\n",
            "P(processing|language) = 0.5000\n",
            "P(.|language) = 0.5000\n",
            "P((|processing) = 1.0000\n",
            "P(nlp|() = 0.5000\n",
            "P(ai|() = 0.5000\n",
            "P()|nlp) = 0.5000\n",
            "P(is|nlp) = 0.5000\n",
            "P(is|)) = 0.5000\n",
            "P(that|)) = 0.5000\n",
            "P(a|is) = 0.2500\n",
            "P(focused|is) = 0.2500\n",
            "P(to|is) = 0.2500\n",
            "P(valuable|is) = 0.2500\n",
            "P(sub-field|a) = 0.5000\n",
            "P(manner|a) = 0.5000\n",
            "P(of|sub-field) = 1.0000\n",
            "P(artificial|of) = 0.3333\n",
            "P(nlp|of) = 0.3333\n",
            "P(the|of) = 0.3333\n",
            "P(intelligence|artificial) = 1.0000\n",
            "P((|intelligence) = 1.0000\n",
            "P()|ai) = 1.0000\n",
            "P(is|that) = 1.0000\n",
            "P(on|focused) = 1.0000\n",
            "P(the|on) = 1.0000\n",
            "P(interaction|the) = 0.3333\n",
            "P(ultimate|the) = 0.3333\n",
            "P(human|the) = 0.3333\n",
            "P(between|interaction) = 1.0000\n",
            "P(computers|between) = 1.0000\n",
            "P(and|computers) = 1.0000\n",
            "P(humans|and) = 0.5000\n",
            "P(make|and) = 0.5000\n",
            "P(through|humans) = 1.0000\n",
            "P(natural|through) = 1.0000\n",
            "P(the|.) = 1.0000\n",
            "P(objective|ultimate) = 1.0000\n",
            "P(of|objective) = 1.0000\n",
            "P(read|to) = 1.0000\n",
            "P(,|read) = 1.0000\n",
            "P(decipher|,) = 0.3333\n",
            "P(understand|,) = 0.3333\n",
            "P(and|,) = 0.3333\n",
            "P(,|decipher) = 1.0000\n",
            "P(,|understand) = 1.0000\n",
            "P(sense|make) = 1.0000\n",
            "P(of|sense) = 1.0000\n",
            "P(languages|human) = 1.0000\n",
            "P(in|languages) = 1.0000\n",
            "P(a|in) = 1.0000\n",
            "P(that|manner) = 1.0000\n",
            "P(.|valuable) = 1.0000\n",
            "\n",
            "Predicted next word after 'natural': language\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text corpus\n",
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a sub-field of artificial intelligence (AI) that is focused on the interaction between computers and humans through natural language.\n",
        "The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Generate unigrams (single words)\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "unigram_freq = Counter(unigrams)\n",
        "print(\"Unigrams:\")\n",
        "print(unigram_freq)\n",
        "\n",
        "# Generate bigrams (pairs of words)\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "bigram_freq = Counter(bigrams)\n",
        "print(\"\\nBigrams:\")\n",
        "print(bigram_freq)\n",
        "\n",
        "# Generate trigrams (triplets of words)\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "trigram_freq = Counter(trigrams)\n",
        "print(\"\\nTrigrams:\")\n",
        "print(trigram_freq)\n",
        "\n",
        "# Calculate bigram probabilities\n",
        "bigram_prob = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for (w1, w2) in bigrams:\n",
        "    bigram_prob[w1][w2] += 1\n",
        "\n",
        "for w1 in bigram_prob:\n",
        "    total_count = float(sum(bigram_prob[w1].values()))\n",
        "    for w2 in bigram_prob[w1]:\n",
        "        bigram_prob[w1][w2] /= total_count\n",
        "\n",
        "print(\"\\nBigram Probabilities:\")\n",
        "for w1 in bigram_prob:\n",
        "    for w2 in bigram_prob[w1]:\n",
        "        print(f\"P({w2}|{w1}) = {bigram_prob[w1][w2]:.4f}\")\n",
        "\n",
        "# Predict the next word based on the current word\n",
        "def predict_next_word(current_word):\n",
        "    if current_word in bigram_prob:\n",
        "        next_words = list(bigram_prob[current_word].keys())\n",
        "        probabilities = list(bigram_prob[current_word].values())\n",
        "        return random.choices(next_words, probabilities)[0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Example of next word prediction\n",
        "current_word = 'natural'\n",
        "predicted_word = predict_next_word(current_word)\n",
        "print(f\"\\nPredicted next word after '{current_word}': {predicted_word}\")\n"
      ]
    }
  ]
}